ðŸ“˜ 01 â€” Requisitos e EspecificaÃ§Ã£o do Projeto
Plataforma de Dados Multicanal para DetecÃ§Ã£o de Fraudes Financeiras
1. Contexto

A Ã¡rea de risco identificou um aumento no volume de transaÃ§Ãµes e na complexidade dos padrÃµes de fraude.
Para melhorar a identificaÃ§Ã£o de comportamentos suspeitos, a gestÃ£o solicitou a construÃ§Ã£o de uma plataforma moderna de engenharia de dados que integre mÃºltiplas fontes e permita anÃ¡lises avanÃ§adas e modelos de machine learning.

2. Objetivos do Projeto
2.1 Objetivo Geral

Construir uma arquitetura de dados escalÃ¡vel e robusta para ingestÃ£o, armazenamento, processamento e anÃ¡lise de dados relacionados a transaÃ§Ãµes financeiras e eventos suspeitos.

2.2 Objetivos EspecÃ­ficos

Ingerir dados via API, CSV, banco relacional (PostgreSQL) e nÃ£o relacional (MongoDB Atlas).

Implementar pipelines Batch e Streaming.

Criar Data Lake em AWS S3 com camadas Bronze, Silver e Gold.

Processar dados com Spark / PySpark e Databricks.

Orquestrar tudo via Airflow.

Transformar dados e modelar tabelas no Snowflake via DBT.

Criar dashboards com mÃ©tricas de fraude.

Versionar todo o cÃ³digo via GitHub.

3. Escopo
3.1 Escopo de IngestÃ£o

O projeto deve ingerir dados provenientes de:

API de transaÃ§Ãµes financeiras.

Arquivos CSV carregados diariamente.

PostgreSQL (AWS RDS).

MongoDB Atlas conectado Ã  AWS.

Streaming em tempo real (simulado via socket/kafka).

3.2 Escopo de Processamento

Tratamento e limpeza dos dados (Bronze â†’ Silver).

CriaÃ§Ã£o de features e agregaÃ§Ãµes analÃ­ticas (Silver â†’ Gold).

Pipelines batch diÃ¡rios.

Pipelines streaming contÃ­nuos.

Notebooks de validaÃ§Ã£o no Databricks.

3.3 Escopo de Armazenamento

AWS S3 com trÃªs camadas:

Bronze: dados crus.

Silver: dados limpos.

Gold: dados analÃ­ticos.

3.4 Escopo de Modelagem de Dados

Snowflake como Data Warehouse final.

Modelagem via DBT (staging + marts).

CriaÃ§Ã£o de:

dim_customer

dim_card

fact_transactions

fact_fraud_risk

4. Requisitos Funcionais

RF01 â€“ A plataforma deve receber dados de mÃºltiplas fontes.

RF02 â€“ Dados devem ser armazenados no S3 em camadas mÃºltiplas.

RF03 â€“ Pipelines devem ser orquestrados no Airflow.

RF04 â€“ Deve existir um pipeline batch.

RF05 â€“ Deve existir um pipeline streaming.

RF06 â€“ Dados limpos devem ir para o Snowflake.

RF07 â€“ TransformaÃ§Ãµes finais devem ser feitas via DBT.

RF08 â€“ Deve existir logging das execuÃ§Ãµes.

RF09 â€“ Dashboards devem ser gerados com base no Snowflake.

5. Requisitos NÃ£o Funcionais

RNF01 â€“ A soluÃ§Ã£o deve ser escalÃ¡vel.

RNF02 â€“ O sistema deve ser resiliente e recuperÃ¡vel.

RNF03 â€“ O Data Lake deve seguir boas prÃ¡ticas de particionamento.

RNF04 â€“ Dados sensÃ­veis devem ser mascarados.

RNF05 â€“ Todo cÃ³digo deve ser versionado no GitHub.

6. Arquitetura Proposta (VisÃ£o Geral)
                    +------------- API ---------------+
                    |                                 |
                    v                                 |
    +----------+   +---------+    +-------------+     |
    |  CSVs    |-->| Airflow |--->|   S3 Bronze |<----+
    +----------+   +---------+    +-------------+
                      ^   ^               |
                      |   |               v
       +--------------+   +---------+  Spark/Databricks
       |                           |       Silver
+-------------+         +-------------+
| PostgreSQL  |         | MongoDB     |
+-------------+         +-------------+
                             |
                             v
                         S3 Gold
                             |
                             v
                        Snowflake + DBT
                             |
                             v
                          Dashboards

7. EntregÃ¡veis
TÃ©cnicos

CÃ³digo das DAGs Airflow

Scripts PySpark

Notebooks Databricks

Modelos DBT

Scripts SQL (PostgreSQL)

ConfiguraÃ§Ã£o do Data Lake

DocumentaÃ§Ã£o

Documento de requisitos

Documento de arquitetura

README.md profissional

Mapa do fluxo de dados

Guia de execuÃ§Ã£o

8. CritÃ©rios de Aceite

Pipelines Airflow funcionando.

Bronze/Silver/Gold populados.

Streaming ativo.

Snowflake com dados modelados.

Dashboard funcional.

RepositÃ³rio GitHub completo.

9. Riscos

MudanÃ§a de schema na API.

Instabilidade do Databricks Community.

Custos da AWS (usar free tier quando possÃ­vel).

ðŸ”¹ Fim do documento.
