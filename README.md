# ğŸš¨ Fraud Detection Data Platform

Plataforma de dados moderna para detecÃ§Ã£o de fraudes em transaÃ§Ãµes
financeiras, utilizando ingestÃ£o de mÃºltiplas fontes, arquitetura
Lakehouse, processamento distribuÃ­do e modelagem analÃ­tica.

Este projeto demonstra habilidades prÃ¡ticas de **Data Engineering** em
um pipeline **end-to-end**, adequado para apresentaÃ§Ã£o em entrevistas.

------------------------------------------------------------------------

# ğŸ—ï¸ Arquitetura Geral

A plataforma segue o padrÃ£o **Data Lakehouse**, utilizando:

-   AWS S3 como Data Lake\
-   Databricks / Spark para processamento\
-   Snowflake para Data Warehouse\
-   Airflow para orquestraÃ§Ã£o\
-   dbt Cloud para modelagem analÃ­tica

``` mermaid
flowchart LR
    A[Fontes de Dados
API / CSV / PostgreSQL / MongoDB] --> B[Airflow
IngestÃ£o Batch + Streaming]
    B --> C[S3 Data Lake
Bronze Layer]
    C --> D[Databricks / Spark
Processamento]
    D --> E[S3 Data Lake
Silver Layer]
    E --> F[dbt
TransformaÃ§Ãµes AnalÃ­ticas]
    F --> G[Snowflake
Camada Gold]
    G --> H[Dashboards
Power BI / Tableau]
```

------------------------------------------------------------------------

# ğŸ§© Componentes do Projeto

## âœ”ï¸ **IngestÃ£o de Dados (Airflow)**

Fontes integradas: - API (JSON)\
- Arquivos CSV\
- Banco Relacional (PostgreSQL)\
- Banco NÃ£o-Relacional (MongoDB)

Pipelines batch e streaming salvam dados brutos no Data Lake.

------------------------------------------------------------------------

## âœ”ï¸ **Armazenamento -- Data Lake (S3)**

Estruturado pela arquitetura **Medallion**:

### **Bronze**

Dados brutos, sem transformaÃ§Ã£o.

### **Silver**

Limpeza, padronizaÃ§Ã£o, deduplicaÃ§Ã£o.

### **Gold**

Modelagem analÃ­tica / features de fraude.

------------------------------------------------------------------------

## âœ”ï¸ **Processamento -- Databricks / PySpark**

TransformaÃ§Ãµes Bronze â†’ Silver:

-   NormalizaÃ§Ã£o de schema\
-   ConversÃ£o de tipos\
-   Tratamento de nulos\
-   DeduplicaÃ§Ã£o\
-   Enriquecimento\
-   Particionamento

------------------------------------------------------------------------

## âœ”ï¸ **Modelagem AnalÃ­tica -- dbt Cloud**

Silver â†’ Gold:

-   Tabelas fato e dimensÃ£o\
-   Features de fraude\
-   MÃ©tricas agregadas\
-   Testes de qualidade\
-   DocumentaÃ§Ã£o automÃ¡tica

------------------------------------------------------------------------

## âœ”ï¸ **Data Warehouse -- Snowflake**

Armazena:

-   `dim_customer`\
-   `dim_device`\
-   `fact_transactions`\
-   `fact_fraud_features`\
-   MÃ©tricas para dashboards

------------------------------------------------------------------------

## âœ”ï¸ **Dashboards AnalÃ­ticos**

KPIs de fraude:

-   \% transaÃ§Ãµes suspeitas\
-   Score de risco\
-   Anomalias por geolocalizaÃ§Ã£o\
-   Volume por canal\
-   TendÃªncia temporal de ocorrÃªncias

Criados em Power BI ou Tableau.

------------------------------------------------------------------------

# ğŸ”„ Fluxo End-to-End

``` mermaid
sequenceDiagram
    participant S as Fontes
    participant A as Airflow
    participant B as S3 Bronze
    participant C as Databricks
    participant D as S3 Silver
    participant E as dbt
    participant F as Snowflake
    participant G as Dashboard

    S->>A: IngestÃ£o API/CSV/Postgres/MongoDB
    A->>B: Salva dados brutos (Bronze)
    A->>C: Trigger job Spark
    C->>D: Salva dados tratados (Silver)
    A->>E: Executa dbt run + dbt test
    E->>F: PublicaÃ§Ã£o camada Gold
    F->>G: Dashboard lÃª mÃ©tricas
```

------------------------------------------------------------------------

# ğŸ“ Estrutura do RepositÃ³rio

``` bash
fraud-detection-data-platform/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ documentation.md
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ ingest_api.py
â”‚       â”œâ”€â”€ ingest_csv.py
â”‚       â”œâ”€â”€ ingest_postgres.py
â”‚       â”œâ”€â”€ ingest_mongodb.py
â”‚       â”œâ”€â”€ process_databricks.py
â”‚       â””â”€â”€ run_dbt.py
â”‚
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ pyspark_jobs/
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ gold/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ snowflake/
â”‚   â””â”€â”€ postgres/
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ documentation.md
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ setup_guide.md
    â”œâ”€â”€ architecture_explanation.md
    â””â”€â”€ interview_talking_points.md
```

------------------------------------------------------------------------

# ğŸš€ Como Executar o Projeto

## 1. Preparar Infra AWS

-   Criar buckets:
    -   `fraud-detection-lake`
    -   `apache-airflow-slax-bucket`
-   Criar permissÃµes IAM
-   Criar segredos no Secrets Manager

## 2. Airflow

-   Subir as DAGs para o bucket configurado\
-   Ativar ingestÃµes

## 3. Databricks

-   Importar notebooks\
-   Configurar cluster\
-   Criar Jobs Bronze â†’ Silver

## 4. dbt Cloud

-   Conectar ao Snowflake\
-   Executar:

``` bash
dbt run
dbt test
```

## 5. Dashboards

-   Conectar ao Snowflake\
-   Criar KPIs de fraude

------------------------------------------------------------------------

# ğŸ’¡ Pontos importantes para entrevistas

-   Arquitetura Lakehouse\
-   Medallion Architecture\
-   Pipelines batch + streaming\
-   Databricks + PySpark\
-   dbt para governanÃ§a\
-   Uso de Snowflake\
-   Airflow como orquestrador\
-   SeguranÃ§a com Secrets Manager

------------------------------------------------------------------------

# ğŸ§­ Melhorias Futuras

-   Implementar Great Expectations\
-   Criar Feature Store\
-   Implementar CDC (Debezium)\
-   Criar testes automatizados de Spark\
-   Adicionar modelo ML de detecÃ§Ã£o de fraude

------------------------------------------------------------------------

# ğŸ™Œ Contato

Este repositÃ³rio foi criado como parte do meu portfÃ³lio profissional
para demonstrar habilidades de Engenharia de Dados em um projeto
end-to-end.
